# S-Expression Agent System

An explainable agent system prototype using S-expressions (S-expression) for task execution.

## ðŸŒ Languages / è¨€èªž

- ðŸ‡ºðŸ‡¸ **English** (This file)
- ðŸ‡¯ðŸ‡µ **[æ—¥æœ¬èªž (Japanese)](README.md)**
- ðŸ“– **[Other Languages / ãã®ä»–ã®è¨€èªž](docs/README_languages.md)**

## Overview

This system provides an explainable agent architecture that executes tasks through S-expressions generated by LLMs.

### Key Features

- **Explainability**: Explicit execution plans through S-expressions
- **User Control**: Review and edit generated S-expressions
- **Context Awareness**: S-expression evaluation considering overall task context
- **Full MCP Integration**: External tool connectivity via Model Context Protocol (MCP)
- **Execution Tracing**: Detailed execution history and MCP call visualization
- **Auto-Recovery**: Automatic LLM-based regeneration for syntax errors
- **TUI Support**: Rich text user interface with real-time feedback
- **LangChain Integration**: Tracing capabilities via langchain/langgraph

## Setup

### Prerequisites

- Python 3.12+
- uv (Python package manager)
- Local LLM server (OpenAI API compatible)

### Installation

```bash
# Clone the repository
git clone https://github.com/hama-jp/s_style_agent.git
cd s-style-agent

# Install dependencies with uv
uv sync
```

### LLM Configuration

This system supports multiple LLM providers. Configure using environment variables:

#### Local LLM (Default)

```bash
export LLM_BASE_URL="http://localhost:1234/v1"
export LLM_MODEL_NAME="local-model"
export LLM_API_KEY="dummy"
```

#### OpenAI

```bash
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_MODEL_NAME="gpt-4"
export LLM_API_KEY="your-openai-api-key-here"
```

Or for GPT-3.5 Turbo:

```bash
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_MODEL_NAME="gpt-3.5-turbo"
export LLM_API_KEY="your-openai-api-key-here"
```

#### Anthropic Claude

```bash
export LLM_BASE_URL="https://api.anthropic.com"
export LLM_MODEL_NAME="claude-3-sonnet-20240229"
export LLM_API_KEY="your-anthropic-api-key-here"
```

#### Additional Settings

```bash
# LLM behavior settings
export LLM_TEMPERATURE="0.3"  # Creativity level (0.0-1.0)

# System settings
export DEBUG="false"
export LANGSMITH_PROJECT="s-style-agent"
```

### MCP Configuration (Optional)

To use Model Context Protocol (MCP) compatible tools:

1. Copy `mcp.json.example` to create `mcp.json`
2. Configure required API keys
3. Tools will be automatically available at system startup

## Usage

### Basic Usage

```bash
# TUI mode (default, recommended)
uv run python main.py

# Traditional CLI mode (for testing/debugging)
uv run python main.py --cli

# Trace viewer standalone
uv run python main.py --trace-only
```

### TUI Mode (Recommended)

Rich interface with 4 tabs:

1. **Dashboard**: System status & quick actions
2. **Workspace**: S-expression generation, execution & real-time tracing
3. **History**: Session history & tool management
4. **Settings**: LLM, MCP & logging configuration

**Keyboard Shortcuts**:
- `Tab/Shift+Tab`: Switch tabs
- `F1`: Help / `F2`: Generate / `F3`: History / `F4`: Settings
- `F5`: Execute / `F6`: Step / `F7`: Edit / `F8`: Save
- `Ctrl+Q`: Quit

### CLI Mode (Testing)

- `/help` - Display help
- `/generate` - Generate S-expression using LLM
- `/parse` - Check S-expression syntax
- `/execute` - Execute S-expression
- `/trace` - Show execution trace
- `/history` - Show session history
- `/tools` - List available tools
- `/tui` - Switch to TUI mode
- `/exit` - Exit system

### Usage Examples

```
> Calculate 2 plus 3
Generated S-expression: (calc "2 + 3")
Execute? (y/n/e=edit): y
Result: 5

> (seq (notify "Starting") (calc "10 * 5") (notify "Done"))
[NOTIFY] Starting
[NOTIFY] Done
Result: Done
```

## S-Expression Syntax

### Basic Syntax

- `(seq step1 step2 ...)` - Sequential execution
- `(par stepA stepB ...)` - Parallel execution
- `(if cond then else)` - Conditional branching
- `(let ((var expr) ...) body)` - Variable binding

### Available Tools

#### Built-in Tools
- `(notify "message")` - User notification
- `(calc "expression")` - Mathematical calculation (SymPy)
- `(math "expression" "operation" "var")` - Symbolic mathematics
- `(step_math "expression" "operation" "var")` - Step-by-step mathematical solving
- `(ask_user "question" "var_name" "type")` - User interaction

#### MCP Tools (Auto-available)
- `(search "query")` - Brave search engine
- Other MCP provider tools are automatically integrated

#### Advanced Features
- **Auto Error Recovery**: LLM automatically fixes syntax errors in S-expressions
- **Real-time Tracing**: Detailed execution logs and MCP call monitoring
- **Asynchronous Execution**: True parallel processing with `(par ...)`

## Architecture

### Core Components

- **Parser**: S-expression parsing and validation
- **Evaluator**: Context-aware S-expression evaluation
- **Tools**: Extensible tool system for various operations
- **CLI**: Interactive command-line interface
- **API**: REST/WebSocket API server

### Execution Modes

- **Synchronous**: Traditional sequential execution
- **Asynchronous**: True parallel execution with task management

## Development

### Running Tests

```bash
# Run all tests
uv run python -m pytest s_style_agent/tests/

# Run specific test category
uv run python -m pytest s_style_agent/tests/test_mcp_*.py
```

### Project Structure

```
s_style_agent/
â”œâ”€â”€ cli/           # Command-line interface
â”œâ”€â”€ core/          # Core evaluation engine
â”œâ”€â”€ api/           # REST/WebSocket API
â”œâ”€â”€ tools/         # Tool implementations
â”œâ”€â”€ mcp/           # MCP integration
â”œâ”€â”€ config/        # Configuration management
â””â”€â”€ tests/         # Test suite
```

## API Server

### Starting the API Server

```bash
# Start FastAPI server
uv run python -m s_style_agent.api.server
```

### API Endpoints

- `GET /` - Health check
- `POST /parse` - Parse S-expression
- `POST /execute` - Execute S-expression
- `POST /generate` - Generate S-expression from natural language
- `GET /status` - System status

### WebSocket Support

Real-time S-expression execution via WebSocket at `/ws/{session_id}`.

## MCP Integration

The system includes Model Context Protocol integration for extending capabilities:

- **Automatic Discovery**: MCP tools are automatically registered
- **Seamless Integration**: Use MCP tools directly in S-expressions
- **Error Handling**: Robust error handling and recovery

## Security

- No personal information or API keys in source code
- Environment variable based configuration
- Comprehensive `.gitignore` for sensitive files
- Safe execution environment for S-expressions

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes and add tests
4. Submit a pull request

## License

[Add your license information here]

## Latest Features (v1.1.0)

### âœ… Completed
- **MCP Trace Display**: Detailed MCP call status in execution traces
- **Auto Error Recovery**: LLM-based automatic regeneration for S-expression syntax errors
- **TUI Integration**: Rich text user interface with real-time feedback
- **Unified Architecture**: Shared processing foundation for CLI/TUI

### Example (Auto Recovery)

```
Input: (search "curry"  # Missing bracket
â†’ [S-expr Eval] Error - Expected )
â†’ [S-expr Eval] Sending error to LLM for regeneration...
â†’ [S-expr Eval] Regenerated: (search "curry")
â†’ âœ… Success: Search results...
```

## Acknowledgments

This project is inspired by the principles of explainable AI and symbolic computation.

---

ðŸ¤– **Generated with [Claude Code](https://claude.ai/code)**